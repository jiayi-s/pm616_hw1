---
title: "PM616 HW1 Q2"
author: "Sylvia Shen"
date: "2022-09-12"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(keras)
```

*Simulate a small dataset (with n = 1000) with a binary outcome. Fit a logistic regression model and an ANN model without any hidden layer. Compare the results.*

```{r}
set.seed(1)
sample_size = 1000
x1 = rnorm(sample_size, mean = 0, sd = 1)
x2 = rnorm(sample_size, mean = 0, sd = 1)
beta1 = 1
beta2 = 2
eta = beta1*x1 + beta2*x2
linear_pred = 1/(1+exp(-eta))
y = rbinom(sample_size, size = 1, prob = linear_pred)
```

### Logistic Regression 
```{r}
model.glm = glm(y~x1+x2, family = "binomial")
summary(model.glm)
```
### ANN

```{r}
model.ann <- keras_model_sequential() %>% 
  layer_dense(1, activation = "sigmoid")


model.ann %>% compile(
  optimizer='adam', 
  loss='binary_crossentropy', 
  metrics='accuracy'
)

model.ann.fit = model.ann %>% fit(as.matrix(cbind(x1,x2)), 
                          as.matrix(y),
                          # validation_split = 0.2,
                          epochs = 200, batch_size = 50
                             )

summary(model.ann)
ann.weights = get_weights(model.ann)
```

### Comparison 
```{r}
data.frame(parameter = c("bias", "x1", "x2"), 
           truth = c(0,beta1, beta2),
           logistic = as.numeric(coef(model.glm)), 
           ANN = c(ann.weights[[2]], ann.weights[[1]][,1])) %>% 
  knitr::kable(digits = 3)
```
In this simulation, the parameter estimates from the ANN model is very close to those from fitting a GLM. By specifying `epoch = 200` and `batch_size = 50`, the loss stabilized after epoch #150 at around 0.47. But the GLM estimates are still slightly less biased compared to the ANN estimates. This might be due to the fact that the iteratively reweighted least squares (IRLS) algorithm used in `glm()` is more robust in this case than Stochastic Gradient Descent in the ANN.  


